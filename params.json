{"name":"Breacon","tagline":"Breaking the Concurrency: Randomizing the Thread Scheduler to unravel hidden bugs","body":"### Problem Statement\r\n\r\nModern web and distributed applications heavily rely on concurrent execution. As usage of multi-core processors gain momentum, client side programs are also switching to multiple threads in their code. However, even for skilled developers, creating multithreaded software is a challenge. Concurrency issues such as deadlocks and data races are often tough to fix without being able to reproduce them. In a multi-threaded concurrent program, the number of possible interleavings is enormous - exponential to the length of the program and the number of threads .These interleavings depend on the context switches that happened during the run which are very difficult to predict; thus, the probability of reproducing a concurrent failure is extremely low. Stress testing methods such as repeatedly executing the program for different payloads with the hope of finding a bug is not efficient because tests that are independent of I/O latency and network usage will usually lead to the same interleaving. We believe that an \r\n## effective concurrency fuzzer should have the following characteristics:\r\n1. Accuracy - Find valid bugs with particular emphasis on minimizing false positives.\r\n2. Replay Support – A replay algorithm to support reproducibility.\r\n3. Coverage – Test the entire problem space and prove the absence of faults.\r\n4. Extensibility - Plugin based architecture to support enhancements.\r\n\r\nWe did a study of few existing concurrency testing techniques [2] [3] [4] [5] and found that none of them were calibrated to support the aforementioned characteristics. As part of the first phase of our research we want to develop a fuzzer that is capable of finding concurrency bugs with high probability and tries to achieve a balance between the first three characteristics listed above. If time permits, for the second phase we intend to develop an extensible framework for the tool to support customizations.  \r\n\r\n### Our Idea\r\nWe plan to combine different techniques to create a new method of concurrency bug detection. Fuzz testing has proven itself to be a powerful means of detecting bugs. This type of testing provides an application with random input and records the application’s response. Hanging and crashing signify that the application has failed the test. We will build upon these ideas by randomly scheduling threads to uncover concurrency bugs. However, these bugs often occur infrequently, and we can reduce the required time to find bugs by combining the random scheduling with heuristics. Common concurrency bugs including data races and deadlocks are normally caused by the improper use of synchronization primitives or the lack thereof. Without these primitives, sequences of instructions may be executed in any order. Synchronization reduces the number of possible sequences, and when properly used, eliminates all of the error-causing sequences. This knowledge may be paired with fuzz testing to create random sequences that are most likely to result in exhibiting bugs if they exist.\r\n\r\nOur approach will scan the binaries of its target applications to find critical points for concurrency bugs.These points include obtaining and releasing locks, as well as, accessing shared memory. If these points may be executed in any order without changing the output, then there are no concurrency bugs. Our approach also involves adding to the fuzz testing framework to check the output of programs in addition to checking that they do not crash or hang. We selected this approach, because it allows for our tool to learn about applications without requiring source code or developer intervention.\r\n\r\nWe plan to utilize software debugger’s capabilities of stopping program execution to place stalls in threads. By inserting breakpoints throughout the code, we can stop and start threads in such a way that allows us to control the sequencing of instructions. Programs will run multiple times in the debugger with the breakpoints set in different places to test different sequences. If one sequence fails, our tool will record the results and the breakpoints used. This allows the sequence to be recreated and the programmer to understand what went wrong. The tool will automate running tests to further simplify the task of finding bugs.\r\n\r\n### Implementation Plan \r\nWe are currently scheduling our project to be completed in eight full weeks after our initial preparation week. This week, week zero, is devoted to finding and reading related research and gaining a high-level insight into debuggers and program analysis. After this week, we will be able to plan the details of our testing tool and begin our implementation. \r\nWeeks one and two (12. October - 25. October) are proposed for developing the program analysis portion of the tool. We will build an application that can load an executable binary and locate the critical points for concurrency. These points will be stored in a list with their accompanying type. We plan to look into resources regarding compiler construction and dynamic instrumentation to design and develop this portion of the tool.\r\nWeeks three and four (26. October - 8. November) will be used for integrating the debugger into our tool. We will also build our central testing algorithm in this stage. After completing program analysis, the tool will fork a new process and run the debugger. Through inter-process communication, the parent process will operate the debugger by instructing it to run the binary with the appropriate breakpoints. The tool will time the execution of the debugger and monitor the output of the application being tested.\r\n\r\nWeek five and the first half of week six (9. November - 19. November) will be our testing phase. We will run our tool on a variety of concurrent applications, some of which will have known concurrency bugs. If our tool functions properly, it should find the known bugs. We hope to discover some unknown concurrency bugs as well.\r\n\r\nThe second half of week six and week seven (20. November - 29. November) will be used for performing our analysis of our testing and the tool in general. During this time we will write our final report on the project. \r\n\r\nWeek eight is slack time, which will be reserved in the case of exceptional circumstances or unforeseen delays. We will finish the project by 28. November.\r\n\r\n### Evaluation Criteria\r\nThe concurrency bug detection algorithm and replay algorithm will form the core of our tool. The efficacy of heuristics and models applied in the algorithm implementation would decide if our concurrency failure detection technique performs better than existing testing approaches. During the testing phase, we plan to run our fuzzer on variety of multi-threaded applications. It is important that the fuzzer provides code coverage guarantee, discovers valid bugs and reduces the likelihood of false positives.\r\n\r\n### References\r\n[1] O. Edelstein, E. Farchi, Y. Nir, G. Ratsaby, S. Ur, “Multithreaded Java program test generation”,pp1,2002.\r\n[2] Sebastian Burckhardt, Pravesh Kothari, Madanlal Musuvathi “A Randomized Scheduler with Probabilistic Guarantees of Finding Bugs”, March. 2010.\r\n[3] Evgeny Vainer, Amiram Yehudai, “Controlling Concurrent Behavior while Testing Multithreaded Software”,2013.\r\n[4] Pallavi Joshi, Mayur Naik, Chang-Seo Park, and Koushik Sen, “CalFuzzer: An Extensible Active Testing Framework for Concurrent Programs”, 2009\r\n[5] Qingzhou Luo, Sai Zhang, Jianjun Zhao, and Min Hu, “A Lightweight and Portable Approach to Making Concurrent Failures Reproducible”, 2010\r\n\r\n### Authors\r\nAchin Kulshrestha and Alexander Morris.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}